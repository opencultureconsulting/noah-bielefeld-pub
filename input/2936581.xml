<mods xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" version="3.3" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-3.xsd"><id>2936581</id><setSpec>bi_dissertation</setSpec><setSpec>doc-type:doctoralThesis</setSpec><setSpec>ddc:000</setSpec><setSpec>bi_dissertationFtxt</setSpec><setSpec>open_access</setSpec>

<genre>thesis</genre>

<titleInfo><title>Memory Models for Incremental Learning Architectures</title></titleInfo>





<name type="personal">
  <namePart type="given">Viktor</namePart>
  <namePart type="family">Losing</namePart>
  <role><roleTerm type="text">author</roleTerm> </role><identifier type="local">63069292</identifier></name>





<name type="personal">
  
  <namePart type="given">Barbara</namePart>
  
  
  <namePart type="family">Hammer</namePart>
  
  <role> <roleTerm type="text">supervisor</roleTerm> </role>
</name>

<name type="personal">
  
  <namePart type="given">Heiko</namePart>
  
  
  <namePart type="family">Wersing</namePart>
  
  <role> <roleTerm type="text">supervisor</roleTerm> </role>
</name>



<name type="corporate">
  <namePart/>
  <identifier type="local">10037</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>








<abstract lang="eng">Technological advancement leads constantly to an exponential growth of generated data in basically every domain, drastically increasing the burden of data storage and maintenance. Most of the data is instantaneously extracted and available in form of endless streams that contain the most current information. Machine learning methods constitute one fundamental way of processing such data in an automatic way, as they generate models that capture the processes behind the data. They are omnipresent in our everyday life as their applications include personalized advertising, recommendations, fraud detection, surveillance, credit ratings, high-speed trading and smart-home devices. Thereby, batch learning, denoting the offline construction of a static model based on large datasets, is the predominant scheme. However, it is increasingly unfit to deal with the accumulating masses of data in given time and in particularly its static nature cannot handle changing patterns. In contrast, incremental learning constitutes one attractive alternative that is a very natural fit for the current demands. Its dynamic adaptation allows continuous processing of data streams, without the necessity to store all data from the past, and results in always up-to-date models, even able to perform in non-stationary environments. In this thesis, we will tackle crucial research questions in the domain of incremental learning by contributing new algorithms or significantly extending existing ones. Thereby, we consider stationary and non-stationary environments and present multiple real-world applications that showcase merits of the methods as well as their versatility. The main contributions are the following:
One novel approach that addresses the question of how to extend a model for prototype-based algorithms based on cost minimization.
We propose local split-time prediction for incremental decision trees to mitigate the trade-off between adaptation speed versus model complexity and run time.
An extensive survey of the strengths and weaknesses of state-of-the-art methods that provides guidance for choosing a suitable algorithm for a given task.
One new approach to extract valuable information about the type of change in a dataset.
We contribute a biologically inspired architecture, able to handle different types of drift using dedicated memories that are kept consistent.
Application of the novel methods within three diverse real-world tasks, highlighting their robustness and versatility.
Investigation of personalized online models in the context of two real-world applications. 

</abstract>

<relatedItem type="constituent">
  <location>
    <url displayLabel="publishedThesis.pdf">https://pub.uni-bielefeld.de/download/2936581/2936582/publishedThesis.pdf</url>
  </location>
  <physicalDescription><internetMediaType>application/pdf</internetMediaType></physicalDescription>
  <accessCondition type="restrictionOnAccess">no</accessCondition>
</relatedItem>
<originInfo><publisher>Universität Bielefeld</publisher><dateIssued encoding="w3cdtf">2019</dateIssued>
</originInfo>
<language><languageTerm authority="iso639-2b" type="code">eng</languageTerm>
</language>



<relatedItem type="host"><identifier type="urn">urn:nbn:de:0070-pub-29365816</identifier><identifier type="doi">10.4119/unibi/2936581</identifier>
<part>
</part>
</relatedItem>


<dateOther encoding="w3cdtf" type="defenseDate">2019-07-09</dateOther>
<extension>
<bibliographicCitation>
<ama>Losing V. &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld; 2019.</ama>
<chicago>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Losing, Viktor. 2019. &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld.&lt;/div&gt;</chicago>
<lncs> Losing, V.: Memory Models for Incremental Learning Architectures. Universität Bielefeld, Bielefeld (2019).</lncs>
<dgps>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Losing, V. (2019). &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2936581.&lt;/div&gt;</dgps>
<angewandte-chemie>V.  Losing, &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;, Universität Bielefeld, Bielefeld, &lt;strong&gt;2019&lt;/strong&gt;.</angewandte-chemie>
<bio1>Losing V (2019) &lt;br /&gt;&lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;.&lt;br /&gt;Bielefeld: Universität Bielefeld.</bio1>
<harvard1>Losing, V., 2019. &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;, Bielefeld: Universität Bielefeld.</harvard1>
<ieee> V. Losing, &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;,  Bielefeld: Universität Bielefeld, 2019.</ieee>
<apa>Losing, V. (2019).  &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2936581</apa>
<wels>Losing, V. (2019): Memory Models for Incremental Learning Architectures. Bielefeld: Universität Bielefeld.</wels>
<apa_indent>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Losing, V. (2019).  &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2936581&lt;/div&gt;</apa_indent>
<mla>Losing, Viktor. &lt;em&gt;Memory Models for Incremental Learning Architectures&lt;/em&gt;. Bielefeld: Universität Bielefeld, 2019.</mla>
<frontiers>Losing, V. (2019). Memory Models for Incremental Learning Architectures. Bielefeld: Universität Bielefeld.</frontiers>
<default>Losing V (2019) &lt;br /&gt;Bielefeld: Universität Bielefeld.</default>
</bibliographicCitation>
</extension>
<recordInfo><recordIdentifier>2936581</recordIdentifier><recordCreationDate encoding="w3cdtf">2019-07-19T07:44:50Z</recordCreationDate><recordChangeDate encoding="w3cdtf">2019-07-25T12:15:35Z</recordChangeDate>
</recordInfo>
</mods>