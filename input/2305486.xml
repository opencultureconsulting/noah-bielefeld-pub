<mods xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" version="3.3" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-3.xsd"><id>2305486</id><setSpec>bi_dissertation</setSpec><setSpec>doc-type:doctoralThesis</setSpec><setSpec>ddc:004</setSpec><setSpec>bi_dissertationFtxt</setSpec><setSpec>open_access</setSpec>

<genre>thesis</genre>

<titleInfo><title>Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion</title></titleInfo>

  
  
<titleInfo type="alternative">
  
  <title>Video-based action recognition for the natural human-machine interaction</title>
</titleInfo>




<name type="personal">
  <namePart type="given">Nils</namePart>
  <namePart type="family">Hofemann</namePart>
  <role><roleTerm type="text">author</roleTerm> </role></name>





<name type="personal">
  
  <namePart type="given">Jan Nikolaus</namePart>
  
  
  <namePart type="family">Fritsch</namePart>
  
  <role> <roleTerm type="text">supervisor</roleTerm> </role>
</name>



<name type="corporate">
  <namePart/>
  <identifier type="local">18304588</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>








<abstract lang="eng">The question addressed in this thesis is how to make use of gestures in a multimodal human-machine interaction. To interact with current computer systems the human still has to adopt himself to the interfaces of the system. The underlying vision of this work is to enable computer systems to interpret the typical human forms of interaction.
Based on this question and vision the goal of this work was developed: Build a robust recognition of human gestures for a multimodal system. Thus the interactive competences of the system can be broadend and improved.
Focussing this goal this work examines the theoretic basis of the human-machine interaction as well as the practical implementation and evaluation of an automatic recognition of gestures. The guideline for the implementation is a flexible and modular architecture. This work presents novel approaches in taking the symbolic and situational context of gesture into account. Continuative work in deriving humans intent of gestures is depicted.
This work concentrates on the interactional behaviour and gestures of humans, thus mostly deictic and manipulative gestures are considered. Their context is the main aspect of these gestures, it allows reasoning about their meaning and intention.
As mentioned above the gesture recognition system is intended to be part of a multimodal system. This integration is shown by the successful use of the developed system within a mobile, social, and multimodal robot and within an wearable assistant system.
Human-human communication and the field of gestures are in the focus of research for many years. Hence this work gives an overview on communication theory and psychological research relevant for the human-machine interaction. A young and challenging aspect is the research on the topic: "How do children learn the manipulation of objects from their parents." The theory developed by Brand et al. [2002] is the basis for an automatic analysis of parents behaviour presented in this work.
This work is a contribution to the vision of a natural human-machine interaction. The emphasis lies on the video-based recognition of human actions and gestures.</abstract>
<abstract lang="ger">Die Fragestellung, unter der diese Dissertation steht, ist, wie sich das Erkennen von Gestik als sinnvoller und gewinnbringender Teil einer multimodalen Interaktion zwischen Menschen und Maschinen erreichen und nutzen lässt. Die Vision besteht darin, dass sich nicht mehr der Mensch an das System anpassen muss, sondern die üblichen und natürlichen Fähigkeiten und Modalitäten des Menschen vom Computersystem unterstützt werden.
Das in dieser Arbeit angestrebte Ziel hat sich aus der Fragestellung und der Vision entwickelt: Es soll eine robuste sowie fehlerarme Erkennung menschlicher Gesten für multimodale Systeme möglich sein. Die interaktionalen Fähigkeiten eines solchen Systems sollen so erweitert und verbessert werden.
Im Blick auf dieses Ziel beschäftigt sich die Arbeit einerseits mit den Grundlagen der Gestik in der Mensch-Maschine-Interaktion, andererseits wird aber auch die praktische Realisierung und Anwendung einer automatischen Gestenerkennung verwirklicht. Bedingung für die Gestenerkennung ist ein flexibler, modularer Aufbau, der eine gute Adaption an unterschiedliche Bedingungen in verschiedenen Systemen gewährleistet. Des Weiteren werden innovative Lösungen für die Interpretation von Handlungen in ihrem symbolischen und situativen Kontext entwickelt.
Deshalb wird der Fokus dieser Arbeit auf besonders wichtige Gesten der Interaktion gelegt. Das Merkmal dieser deiktischen und manipulativen Gesten ist, dass sie in einem Kontext mit Objekten der Umgebung auftreten und in diesem interpretierbar sind.
Die zwischenmenschliche Kommunikation und insbesondere das umfangreiche Gebiet des gestischen Repertoires des Menschen sind Gegenstand langjähriger wissenschaftlicher Untersuchungen. In dieser Arbeit werden deshalb Betrachtungen und Theorien der Kommunikationsforschung und Psychologie mit einbezogen und im Kontext der MMI diskutiert. Als junges und spannendes Gebiet ist insbesondere die Erforschung des kindlichen Lernens von Objektmanipulationen zu nennen. Theorien dieses Gebiets werden in dieser Arbeit aufgegriffen und mit automatisierten Verfahren untersucht und nachvollzogen. Die Forschungsergebnisse der Psychologie sind von grundlegender Bedeutung für das automatische Interpretieren von Gesten und Erschließen ihrer Intention. Die im Rahmen dieser Dissertation für dieses Themengebiet durchgeführten Untersuchungen konzentrieren sich auf Grundlagenforschungen in diesem für die Informatik neuen und vielversprechenden Gebiet.
Diese Arbeit leistet einen Beitrag zur Vision einer natürlichen Interaktion des Menschen mit Maschinen. Der Schwerpunkt wird auf das Erkennen und Interpretieren von Gesten und Handlungen gelegt.</abstract>

<relatedItem type="constituent">
  <location>
    <url displayLabel="Hofemann_Disputation.pdf">https://pub.uni-bielefeld.de/download/2305486/2305489/Hofemann_Disputation.pdf</url>
  </location>
  <physicalDescription><internetMediaType>application/pdf</internetMediaType></physicalDescription>
  <accessCondition type="restrictionOnAccess">no</accessCondition>
</relatedItem>
<originInfo><publisher>Bielefeld University</publisher><dateIssued encoding="w3cdtf">2006</dateIssued>
</originInfo>
<language><languageTerm authority="iso639-2b" type="code">ger</languageTerm>
</language>

<subject><topic>Gestik , Hand , Mustererkennung , Mensch-Maschine-Kommunikation , Kontextbezogenes System , Multimodales System , Mensch-Roboter-Interaktion , Partikelfilter , Gestenerkennung , Assistenzsystem , Gesture recognition , Particle filter , Condensation , Human-robot interaction , Gestures</topic>
</subject>


<relatedItem type="host"><identifier type="urn">urn:nbn:de:hbz:361-10911</identifier>
<part>
</part>
</relatedItem>


<dateOther encoding="w3cdtf" type="defenseDate">2007-04-19</dateOther>
<extension>
<bibliographicCitation>
<chicago>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofemann, Nils. 2006. &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University.&lt;/div&gt;</chicago>
<lncs> Hofemann, N.: Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion. Bielefeld University, Bielefeld (Germany) (2006).</lncs>
<default>Hofemann N (2006) &lt;br /&gt;Bielefeld (Germany): Bielefeld University.</default>
<mla>Hofemann, Nils. &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University, 2006.</mla>
<dgps>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofemann, N. (2006). &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University.&lt;/div&gt;</dgps>
<frontiers>Hofemann, N. (2006). Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion. Bielefeld (Germany): Bielefeld University.</frontiers>
<ama>Hofemann N. &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University; 2006.</ama>
<ieee> N. Hofemann, &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;,  Bielefeld (Germany): Bielefeld University, 2006.</ieee>
<apa>Hofemann, N. (2006).  &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University.</apa>
<wels>Hofemann, N. (2006): Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion. Bielefeld (Germany): Bielefeld University.</wels>
<angewandte-chemie>N.  Hofemann, &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;, Bielefeld University, Bielefeld (Germany), &lt;strong&gt;2006&lt;/strong&gt;.</angewandte-chemie>
<bio1>Hofemann N (2006) &lt;br /&gt;&lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;.&lt;br /&gt;Bielefeld (Germany): Bielefeld University.</bio1>
<harvard1>Hofemann, N., 2006. &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;, Bielefeld (Germany): Bielefeld University.</harvard1>
<apa_indent>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofemann, N. (2006).  &lt;em&gt;Videobasierte Handlungserkennung für die natürliche Mensch-Maschine-Interaktion&lt;/em&gt;. Bielefeld (Germany): Bielefeld University.&lt;/div&gt;</apa_indent>
</bibliographicCitation>
</extension>
<recordInfo><recordIdentifier>2305486</recordIdentifier><recordCreationDate encoding="w3cdtf">2007-05-21T08:56:22Z</recordCreationDate><recordChangeDate encoding="w3cdtf">2020-03-11T08:43:33Z</recordChangeDate>
</recordInfo>
</mods>