<mods xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" version="3.3" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-3.xsd"><id>2933040</id><setSpec>bi_dissertation</setSpec><setSpec>doc-type:doctoralThesis</setSpec><setSpec>ddc:620</setSpec><setSpec>bi_dissertationFtxt</setSpec><setSpec>open_access</setSpec>

<genre>thesis</genre>

<titleInfo><title>Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior</title></titleInfo>





<name type="personal">
  <namePart type="given">Edoardo</namePart>
  <namePart type="family">Casapietra</namePart>
  <role><roleTerm type="text">author</roleTerm> </role><identifier type="local">63069411</identifier></name>





<name type="personal">
  
  <namePart type="given">Franz</namePart>
  
  
  <namePart type="family">Kummert</namePart>
  
  <role> <roleTerm type="text">supervisor</roleTerm> </role>
</name>



<name type="corporate">
  <namePart/>
  <identifier type="local">10036</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>

<name type="corporate">
  <namePart/>
  <identifier type="local">5408013</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>








<abstract lang="eng">The detection of road layout and semantics is an important issue in modern Advanced Driving Assistance Systems (ADAS) and autonomous driving systems. In particular, trajectory planning algorithms need a road representation to operate on: this representation has to be spatial, as the system needs to know exactly on which areas it is safe to drive, so that they can safely plan fine maneuvers. Since typical trajectories are computed for timespans in the order of seconds, the spatial detection range needed for the road representation to achieve a stable and smooth trajectory is in the tenths to hundreds of meters. Direct detection, i.e. the usage of sensors that detect road area by direct observation (e.g. cameras or lasers), is often not sufficient to achieve
this range, especially in inner-city, due to occlusions caused by various obstacles (e.g. buildings and high traffic) as well as hardware limitations. State-of-the-art systems cope with this problem by employing annotated road maps to complement direct detection. However, maps are expensive to make and not available on every road. Furthermore, ego-localization is a key issue in their usage.
This thesis presents a novel approach that creates a spatial road representation derived from both direct and indirect road detection, i.e. the detection and interpretation of other cues for the purpose of inferring the road area layout. Direct detection on monocular images is provided by RTDS, a feature-based detection system that provides road terrain confidence. Indirect detection is based on the interpretation of the other vehicles' behavior. Since our main assumption is that vehicles move on road area, we estimate their past and future movements to infer the road layout where we cannot see it directly. The estimation is carried out using a function that models the probability for each vehicle to traverse each patch of the representation, taking into account position, direction and speed of the vehicle, as well as the possibility of small past and future maneuvers. The behavior of each vehicle is used not only to infer the area where road is, but also to infer where there is not. In fact, observing a vehicle steering away from an area it was predicted to go can be interpreted as evidence that said area is not road.
The road confidences provided by RTDS and behavior interpretation are blended together by means of a visibility function that gives different weights to the two sources, according to the position of the patch in the field of view and possible occlusions that would prevent the camera to see the patch, thereby leading to unreliable results from RTDS.
The addition of indirect detection improves the spatial range of the representation. It also exploits the scenarios of high traffic that are the most challenging ones for direct detection systems, and allows for the inclusion of additional semantics, such as lanes and driving directions. Geometrical considerations are applied to the road layout, obtaining a distributed measure of road width and orientation. These values are used to segment the road, and each segment is then divided into multiple lanes based on its width and the average width of a lane. Finally, a driving direction is assigned to each lane by observing the behavior of the other vehicles on it.
The road representation is evaluated by comparison with a ground truth obtained from manually annotated real world images. As in most cases the entirety of road area cannot be seen in a single image (a problem that human users share with direct detection systems), every road is annotated in multiple different images, and the road portions observed are converted into BEV and fused together using GPS to form a comprehensive view of said road. This ground truth is then compared patch-wise to the representation obtained by our system, showing a clear improvement with respect to the representation obtained by RTDS alone.
In order to demonstrate the advantages of our approach in concrete applications, we set up a system that couples our road representation with a basic trajectory planner. The system reads real-world data, recorded by a mobile platform. The representation is computed at each frame of the stream. The trajectory planner receives the current state of the ego-car (position, direction and speed) and the location of a target area (from a navigational map), and finds the path that leads to the target area with minimum cost.
We show that indirect road detection complements direct detection in a way that leads to a substantial increase in spatial detection range and quality of the internal road representation, thereby improving the smoothness of trajectories that planners can compute, as well as their robustness over time, since the road layout in the representation does not dramatically change only when a new road is visible. This result can help autonomous driving systems to achieve a more human-like behavior, as their improved road awareness allows them to plan ahead, including areas they do not see yet, just as humans normally do.</abstract>

<relatedItem type="constituent">
  <location>
    <url displayLabel="thesis_gedruckt.pdf">https://pub.uni-bielefeld.de/download/2933040/2933041/thesis_gedruckt.pdf</url>
  </location>
  <physicalDescription><internetMediaType>application/pdf</internetMediaType></physicalDescription>
  <accessCondition type="restrictionOnAccess">no</accessCondition>
</relatedItem>
<originInfo><publisher>Universität Bielefeld</publisher><dateIssued encoding="w3cdtf">2019</dateIssued>
</originInfo>
<language><languageTerm authority="iso639-2b" type="code">eng</languageTerm>
</language>



<relatedItem type="host"><identifier type="urn">urn:nbn:de:0070-pub-29330403</identifier><identifier type="doi">10.4119/unibi/2933040</identifier>
<part>
</part>
</relatedItem>


<dateOther encoding="w3cdtf" type="defenseDate">2018-11-23</dateOther>
<extension>
<bibliographicCitation>
<chicago>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Casapietra, Edoardo. 2019. &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld.&lt;/div&gt;</chicago>
<lncs> Casapietra, E.: Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior. Universität Bielefeld, Bielefeld (2019).</lncs>
<default>Casapietra E (2019) &lt;br /&gt;Bielefeld: Universität Bielefeld.</default>
<mla>Casapietra, Edoardo. &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld, 2019.</mla>
<frontiers>Casapietra, E. (2019). Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior. Bielefeld: Universität Bielefeld.</frontiers>
<dgps>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Casapietra, E. (2019). &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2933040.&lt;/div&gt;</dgps>
<ama>Casapietra E. &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld; 2019.</ama>
<ieee> E. Casapietra, &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;,  Bielefeld: Universität Bielefeld, 2019.</ieee>
<apa>Casapietra, E. (2019).  &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2933040</apa>
<wels>Casapietra, E. (2019): Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior. Bielefeld: Universität Bielefeld.</wels>
<angewandte-chemie>E.  Casapietra, &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;, Universität Bielefeld, Bielefeld, &lt;strong&gt;2019&lt;/strong&gt;.</angewandte-chemie>
<bio1>Casapietra E (2019) &lt;br /&gt;&lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;.&lt;br /&gt;Bielefeld: Universität Bielefeld.</bio1>
<harvard1>Casapietra, E., 2019. &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;, Bielefeld: Universität Bielefeld.</harvard1>
<apa_indent>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Casapietra, E. (2019).  &lt;em&gt;Spatial Road Representation for Driving in Complex Scenes by Interpretation of Traffic Behavior&lt;/em&gt;. Bielefeld: Universität Bielefeld. doi:10.4119/unibi/2933040&lt;/div&gt;</apa_indent>
</bibliographicCitation>
</extension>
<recordInfo><recordIdentifier>2933040</recordIdentifier><recordCreationDate encoding="w3cdtf">2019-01-13T13:47:20Z</recordCreationDate><recordChangeDate encoding="w3cdtf">2019-01-23T13:24:26Z</recordChangeDate>
</recordInfo>
</mods>