<mods xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" version="3.3" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-3.xsd"><id>1894565</id><setSpec>conference</setSpec><setSpec>doc-type:conferenceObject</setSpec><setSpec>ddc:006</setSpec><setSpec>conferenceFtxt</setSpec><setSpec>open_access</setSpec>

<genre>conference paper</genre>

<titleInfo><title>Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications</title></titleInfo>


<note type="publicationStatus">published</note>



<name type="personal">
  <namePart type="given">Thies</namePart>
  <namePart type="family">Pfeiffer</namePart>
  <role><roleTerm type="text">author</roleTerm> </role><identifier type="local">182991</identifier><description xsi:type="identifierDefinition" type="orcid">0000-0001-6619-749X</description></name>
<name type="personal">
  <namePart type="given">Nikita</namePart>
  <namePart type="family">Mattar</namePart>
  <role><roleTerm type="text">author</roleTerm> </role><identifier type="local">191149</identifier></name>







<name type="corporate">
  <namePart/>
  <identifier type="local">10044</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>

<name type="corporate">
  <namePart/>
  <identifier type="local">8014655</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>

<name type="corporate">
  <namePart/>
  <identifier type="local">27003888</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>








<abstract lang="eng">The "Where?" is quite important for Mixed Reality applications: Where is the user looking at? Where should augmentations be displayed? The location of the overt visual attention of the user can be used both to disambiguate referent objects and to inform an intelligent view management of the user interface. While the vertical and horizontal orientation of attention is quite commonly used, e.g. derived from the orientation of the head, only knowledge about the distance allows for an intrinsic measurement of the location of the attention. This contribution reviews our latest results on detecting the location of attention in 3D space using binocular eye tracking.</abstract>

<relatedItem type="constituent">
  <location>
    <url displayLabel="Overt_Visual_Attention_in_Mixed_Reality.pdf">https://pub.uni-bielefeld.de/download/1894565/2487426/Overt_Visual_Attention_in_Mixed_Reality.pdf</url>
  </location>
  <physicalDescription><internetMediaType>application/pdf</internetMediaType></physicalDescription>
  <accessCondition type="restrictionOnAccess">no</accessCondition>
</relatedItem>
<originInfo><publisher>Logos Berlin</publisher><dateIssued encoding="w3cdtf">2009</dateIssued>
</originInfo>
<language><languageTerm authority="iso639-2b" type="code">eng</languageTerm>
</language>

<subject><topic>Gaze-based Interaction</topic>
</subject>


<relatedItem type="host"><titleInfo><title>Workshop-Proceedings der Tagung Mensch &amp; Computer 2009: Grenzenlos frei!?</title></titleInfo><identifier type="urn">urn:nbn:de:0070-pub-18945651</identifier>
<part>
</part>
</relatedItem>


<extension>
<bibliographicCitation>
<lncs> Pfeiffer, T., Mattar, N.: Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?  Logos Berlin, Berlin (2009).</lncs>
<chicago>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Pfeiffer, Thies, and Mattar, Nikita. 2009. “Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications”. In &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;. Berlin: Logos Berlin.&lt;/div&gt;</chicago>
<mla>Pfeiffer, Thies, and Mattar, Nikita. “Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications”. &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;. Berlin: Logos Berlin, 2009.</mla>
<default>Pfeiffer T, Mattar N (2009)  &lt;br /&gt;In:  Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?. Berlin: Logos Berlin.</default>
<wels>Pfeiffer, T.; Mattar, N. (2009): Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. In: Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?. Berlin: Logos Berlin.</wels>
<bio1>Pfeiffer T, Mattar N (2009) &lt;br /&gt;Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. &lt;br /&gt;In:  Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?.  Berlin: Logos Berlin.</bio1>
<harvard1>Pfeiffer, T., &amp;amp; Mattar, N., 2009. Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. In &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;.  Berlin: Logos Berlin.</harvard1>
<apa_indent>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Pfeiffer, T., &amp;amp; Mattar, N. (2009). Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt; Berlin: Logos Berlin.&lt;/div&gt;</apa_indent>
<angewandte-chemie>T.  Pfeiffer, and N.  Mattar, in &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;, Logos Berlin, Berlin, &lt;strong&gt;2009&lt;/strong&gt;.</angewandte-chemie>
<ama>Pfeiffer T, Mattar N. Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. In:  &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;. Berlin: Logos Berlin;  2009.</ama>
<frontiers>Pfeiffer, T., and Mattar, N. (2009). “Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications” in &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt; (Berlin: Logos Berlin).</frontiers>
<dgps>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Pfeiffer, T. &amp;amp; Mattar, N. (2009). Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;. Berlin: Logos Berlin.&lt;/div&gt;</dgps>
<apa>Pfeiffer, T., &amp;amp; Mattar, N. (2009). Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications. &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt; Berlin: Logos Berlin.</apa>
<ieee> T. Pfeiffer and N. Mattar, “Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications”, &lt;em&gt;Workshop-Proceedings der Tagung Mensch &amp;amp; Computer 2009: Grenzenlos frei!?&lt;/em&gt;,  Berlin: Logos Berlin, 2009.</ieee>
</bibliographicCitation>
</extension>
<recordInfo><recordIdentifier>1894565</recordIdentifier><recordCreationDate encoding="w3cdtf">2010-11-03T10:10:29Z</recordCreationDate><recordChangeDate encoding="w3cdtf">2018-07-24T12:59:04Z</recordChangeDate>
</recordInfo>
</mods>