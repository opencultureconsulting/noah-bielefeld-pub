<mods xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" version="3.3" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-3.xsd"><id>2902065</id><setSpec>bi_dissertation</setSpec><setSpec>doc-type:doctoralThesis</setSpec><setSpec>ddc:000</setSpec><setSpec>bi_dissertationFtxt</setSpec><setSpec>open_access</setSpec>

<genre>thesis</genre>

<titleInfo><title>Learning vector quantization for proximity data</title></titleInfo>





<name type="personal">
  <namePart type="given">Daniela</namePart>
  <namePart type="family">Hofmann</namePart>
  <role><roleTerm type="text">author</roleTerm> </role><identifier type="local">30516279</identifier></name>





<name type="personal">
  
  <namePart type="given">Barbara</namePart>
  
  
  <namePart type="family">Hammer</namePart>
  
  <role> <roleTerm type="text">supervisor</roleTerm> </role>
</name>



<name type="corporate">
  <namePart/>
  <identifier type="local">10037</identifier>
  <role>
    <roleTerm type="text">department</roleTerm>
  </role>
</name>








<abstract lang="eng">Prototype-based classifiers such as learning vector quantization (LVQ) often display intuitive and flexible classification and learning rules. However, classical techniques are restricted to vectorial data only, and hence not suited for more complex data structures. Therefore, a few extensions of diverse LVQ variants to more general data which are characterized based on pairwise similarities or dissimilarities only have been proposed recently in the literature.&lt;br /&gt;
In this contribution, we propose a novel extension of LVQ to similarity data which is based on the kernelization of an underlying probabilistic model: kernel robust soft LVQ (KRSLVQ). Relying on the notion of a pseudo-Euclidean embedding of proximity data, we put this specific approach as well as existing alternatives into a general framework which characterizes different fundamental possibilities how to extend LVQ towards proximity data: the main characteristics are given by the choice of the cost function, the interface to the data in terms of similarities or dissimilarities, and the way in which optimization takes place. In particular the latter strategy highlights the difference of popular kernel approaches versus so-called relational approaches.
&lt;br /&gt;While KRSLVQ and alternatives lead to state of the art results, these extensions have two drawbacks as compared to their vectorial counterparts: (i) a quadratic training complexity is encountered due to the dependency of the methods on the full proximity matrix; (ii) prototypes are no longer given by vectors but they are represented in terms of an implicit linear combination of data, i.e. interpretability of the prototypes is lost. 
&lt;br /&gt;We investigate different techniques to deal with these challenges: We consider a speed-up of training by means of low rank approximations of the Gram matrix by its Nyström approximation. In benchmarks, this strategy is successful if the considered data are intrinsically low-dimensional. We propose a quick check to efficiently test this property prior to training. 
&lt;br /&gt;We extend KRSLVQ by sparse approximations of the prototypes: instead of the full coefficient vectors, few exemplars which represent the prototypes can be directly inspected by practitioners in the same way as data. We compare different paradigms based on which to infer a sparse approximation: sparsity priors while training, geometric approaches including orthogonal matching pursuit and core techniques, and heuristic approximations based on the coefficients or proximities.
&lt;br /&gt;We demonstrate the performance of these LVQ techniques for benchmark data, reaching state of the art results. We discuss the behavior of the methods to enhance performance and interpretability as concerns quality, sparsity, and representativity, and we propose different measures how to quantitatively evaluate the performance of the approaches. 
&lt;br /&gt;We would like to point out that we had the possibility to present our findings in international publication organs including three journal articles [6, 9, 2], four conference papers [8, 5, 7, 1] and two workshop contributions [4, 3].
&lt;br /&gt;&lt;br /&gt;References
&lt;br /&gt;[1] A. Gisbrecht, D. Hofmann, and B. Hammer. Discriminative dimensionality reduction mappings. Advances in Intelligent Data Analysis, 7619: 126–138, 2012.
&lt;br /&gt;[2] B. Hammer, D. Hofmann, F.-M. Schleif, and X. Zhu. Learning vector quantization for (dis-)similarities. Neurocomputing, 131: 43–51, 2014.
&lt;br /&gt;[3] D. Hofmann. Sparse approximations for kernel robust soft lvq. Mittweida Workshop on Computational Intelligence, 2013.
&lt;br /&gt;[4] D. Hofmann, A. Gisbrecht, and B. Hammer. Discriminative probabilistic prototype based models in kernel space. New Challenges in Neural Computation, TR Machine Learning Reports, 2012.
&lt;br /&gt;[5] D. Hofmann, A. Gisbrecht, and B. Hammer. Efficient approximations of kernel robust soft lvq. Workshop on Self-Organizing Maps, 198: 183–192, 2012.
&lt;br /&gt;[6] D. Hofmann, A. Gisbrecht, and B. Hammer. Efficient approximations of robust soft learning vector quantization for non-vectorial data. Neurocomputing, 147: 96–106, 2015.
&lt;br /&gt;[7] D. Hofmann and B. Hammer. Kernel robust soft learning vector quantization. Artificial Neural Networks in Pattern Recognition, 7477: 14–23, 2012.
&lt;br /&gt;[8] D. Hofmann and B. Hammer. Sparse approximations for kernel learning vector quantization. European Symposium on Artificial Neural Networks, 549–554, 2013.
&lt;br /&gt;[9] D. Hofmann, F.-M. Schleif, B. Paaßen, and B. Hammer. Learning interpretable kernelized prototype-based models. Neurocomputing, 141: 84–96, 2014.</abstract>

<relatedItem type="constituent">
  <location>
    <url displayLabel="thesis_dhofmann.pdf">https://pub.uni-bielefeld.de/download/2902065/2902066/thesis_dhofmann.pdf</url>
  </location>
  <physicalDescription><internetMediaType>application/pdf</internetMediaType></physicalDescription>
  <accessCondition type="restrictionOnAccess">no</accessCondition>
</relatedItem>
<originInfo><publisher>Universität Bielefeld</publisher><dateIssued encoding="w3cdtf">2016</dateIssued>
</originInfo>
<language><languageTerm authority="iso639-2b" type="code">eng</languageTerm>
</language>



<relatedItem type="host"><identifier type="urn">urn:nbn:de:hbz:361-29020658</identifier>
<part><extent unit="pages">101</extent>
</part>
</relatedItem>


<dateOther encoding="w3cdtf" type="defenseDate">2015-10-23</dateOther>
<extension>
<bibliographicCitation>
<mla>Hofmann, Daniela. &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld, 2016.</mla>
<default>Hofmann D (2016) &lt;br /&gt;Bielefeld: Universität Bielefeld.</default>
<aps> D.  Hofmann, Learning vector quantization for proximity data, (Universität Bielefeld, Bielefeld, 2016).</aps>
<lncs> Hofmann, D.: Learning vector quantization for proximity data. Universität Bielefeld, Bielefeld (2016).</lncs>
<chicago>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofmann, Daniela. 2016. &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld.&lt;/div&gt;</chicago>
<apa>Hofmann, D. (2016).  &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld.</apa>
<ieee> D. Hofmann, &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;,  Bielefeld: Universität Bielefeld, 2016.</ieee>
<ama>Hofmann D. &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld; 2016.</ama>
<frontiers>Hofmann, D. (2016). Learning vector quantization for proximity data. Bielefeld: Universität Bielefeld.</frontiers>
<dgps>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofmann, D. (2016). &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld.&lt;/div&gt;</dgps>
<harvard1>Hofmann, D., 2016. &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;, Bielefeld: Universität Bielefeld.</harvard1>
<bio1>Hofmann D (2016) &lt;br /&gt;&lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;.&lt;br /&gt;Bielefeld: Universität Bielefeld.</bio1>
<apa_indent>&lt;div style="text-indent:-25px; padding-left:25px;padding-bottom:0px;"&gt;Hofmann, D. (2016).  &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;. Bielefeld: Universität Bielefeld.&lt;/div&gt;</apa_indent>
<angewandte-chemie>D.  Hofmann, &lt;em&gt;Learning vector quantization for proximity data&lt;/em&gt;, Universität Bielefeld, Bielefeld, &lt;strong&gt;2016&lt;/strong&gt;.</angewandte-chemie>
<wels>Hofmann, D. (2016): Learning vector quantization for proximity data. Bielefeld: Universität Bielefeld.</wels>
</bibliographicCitation>
</extension>
<recordInfo><recordIdentifier>2902065</recordIdentifier><recordCreationDate encoding="w3cdtf">2016-04-01T05:22:14Z</recordCreationDate><recordChangeDate encoding="w3cdtf">2018-11-05T15:30:28Z</recordChangeDate>
</recordInfo>
</mods>