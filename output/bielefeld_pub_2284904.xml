<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2284904">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>(Beyond) referential mechanisms in spatial language comprehension</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Michele</namePart>
                        <namePart type="family">Burigo</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Pia</namePart>
                        <namePart type="family">Knoeferle</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Pia</namePart>
                        <namePart type="family">Knoeferele</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Michele</namePart>
                        <namePart type="family">Burigo</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Maria Nella</namePart>
                        <namePart type="family">Carminati</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Helene</namePart>
                        <namePart type="family">Kreysa</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">conferenceObject</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2011</dateIssued>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">Everyday interaction often involves comprehending spatial language such as when trying to check the time and being told “the clock is on the table”. It is well established that spatial language processing requires attention mechanisms (Carlson &amp; Logan, 2005; Logan, 1994) but how precisely people deploy visual attention during real-time spatial language comprehension, is still unclear.
The Attention Vector Sum (AVS) model postulates that to comprehend sentence (1) people must shift their attention from the vase (‘reference object’) to the clock (located object, e.g., Carlson-Radvansky &amp; Irwin, 1994; Carlson &amp; Logan, 2005, Regier &amp; Carlson, 2001). An alternative account from ‘visual world’ studies suggests people incrementally inspect objects as they are mentioned and thus for (1) inspect the clock followed by the vase (e.g., Tanenhaus et al., 1995). In sum, these two accounts predict opposing inspection orders although the visual world (but not the AVS) account specifies the time course of visual attention allocation. We examine gaze pattern to objects during spatial language comprehension, and evaluate their fit against predictions of the AVS (reference object -&gt; located object) and visual world (located object -&gt; reference object) accounts.
(1) “The clock is above the vase”.
We recorded eye movements while people listened to spatial descriptions (e.g., (1)) and verified whether the sentence matched (vs. didn’t match) the picture. We analysed fixations and inspections (consecutive fixations to an object) for the matching picture-sentence pairs. Shortly after people heard “above” they fixated the vase more often than the clock, corroborating the visual world account. In contrast, analyses of inspections show that people after hearing “above”, and after one inspection to the vase, look next to the clock on 70 percent of inspections The distribution of fixations during “the vase” confirms this view, in that 30 percent of fixations are directed at the clock (vs. 60 percent to the vase vs. 10 percent to a third unrelated distracter object). In sum, gaze analyses after “above” revealed that (a) people anticipate the post-verbal object as predicted by the visual world account, but (b) after inspecting the vase, they next inspect the clock. While the AVS model cannot accommodate findings (a), the visual world account alone cannot accommodate findings (b), suggesting we need aspects of both accounts to accommodate the data.</abstract>
                    <subject>
                        <topic>eye-tracking</topic>
                        <topic>language comprehension</topic>
                        <topic>semantics</topic>
                    </subject>
                    <classification authority="ddc">150</classification>
                    <identifier type="urn">urn:nbn:de:0070-pub-22849048</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2284904</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>conference_object</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2284904">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2284904/2284920/BurigoKnoeferle_ESLP2011.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2284904" DMDID="DMD2284904">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2284904"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
