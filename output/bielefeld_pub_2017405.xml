<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2017405">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Sound and Meaning in Auditory Data Display</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Thomas</namePart>
                        <namePart type="family">Hermann</namePart>
                        <nameIdentifier type="orcid" typeURI="http://orcid.org">0000-0001-7975-4363</nameIdentifier>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Helge</namePart>
                        <namePart type="family">Ritter</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">article</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2004</dateIssued>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">Auditory data display is an interdisciplinary field linking auditory perception research, sound engineering, data mining, and human-computer interaction in order to make semantic contents of data perceptually accessible in the form of (nonverbal) audible sound. For this goal it is important to understand the different ways in which sound can encode meaning. We discuss this issue from the perspectives of language, music, functionality, listening modes, and physics, and point out some limitations of current techniques for auditory data display, in particular when targeting high-dimensional data sets. As a promising, potentially very widely applicable approach, we discuss the method of model-based sonification (MBS) introduced recently by the authors and point out how its natural semantic grounding in the physics of a sound generation process supports the design of sonifications that are accessible even to untrained, everyday listening. We then proceed to show that MBS also facilitates the design of an intuitive, active navigation through &quot;acoustic aspects&quot;, somewhat analogous to the use of successive two-dimensional views in three-dimensional visualization. Finally, we illustrate the concept with a first prototype of a &quot;tangible&quot; sonification interface which allows us to &quot;perceptually map&quot; sonification responses into active exploratory hand motions of a user, and give an outlook on some planned extensions.</abstract>
                    <subject>
                        <topic>thermann</topic>
                    </subject>
                    <classification authority="ddc">004</classification>
                    <relatedItem type="host">
                        <titleInfo>
                            <title>Proceedings of the IEEE (Special Issue on Engineering and Music - Supervisory Control and Auditory Communication)</title>
                        </titleInfo>
                        <part>
                            <detail type="volume">
                                <number>92</number>
                            </detail>
                            <detail type="issue">
                                <number>4</number>
                            </detail>
                            <extent unit="page">
                                <start>730</start>
                                <end>741</end>
                            </extent>
                        </part>
                    </relatedItem>
                    <identifier type="issn">0018-9219</identifier>
                    <identifier type="ISI">000220514500013</identifier>
                    <identifier type="urn">urn:nbn:de:0070-pub-20174054</identifier>
                    <identifier type="doi">10.1109/JPROC.2004.825904</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2017405</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>oaArticle</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2017405">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2017405/2914406/HermannRitter2004-SAM.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2017405" DMDID="DMD2017405">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2017405"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
