<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2409929">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Embodied Behavior Processing in ECAs by Perception-Action Integration</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Amir</namePart>
                        <namePart type="family">Sadeghipour</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Stefan</namePart>
                        <namePart type="family">Kopp</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">conferenceObject</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2011</dateIssued>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">Perception and generation of verbal and nonverbal behavior is one of the main foundations of human social interaction. We model these abilities for embodied conversational agents (ECAs) on the basis of perception-action links as in humans. With a focus on gesture processing, we propose a computational model which enables ECAs to interact with humans in an embodied manner and supports many aspects of social interaction. The model performance is briefly illustrated on the basis of an interaction scene.</abstract>
                    <subject>
                        <topic>Perception-action Links</topic>
                        <topic>Gestures</topic>
                        <topic>Social Embodiment</topic>
                        <topic>Cognitive Model</topic>
                    </subject>
                    <classification authority="ddc">006</classification>
                    <identifier type="isbn">978-1-4503-0268-5/11/0</identifier>
                    <identifier type="urn">urn:nbn:de:0070-pub-24099296</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2409929</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>conference_object</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2409929">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2409929/2409943/SadeghipourKopp.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2409929" DMDID="DMD2409929">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2409929"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
