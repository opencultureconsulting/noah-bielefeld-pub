<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2305667">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Audio-visual emotion recognition for natural human-robot interaction</title>
                    </titleInfo>
                    <titleInfo type="alternative">
                        <title>Audio-Visuelle Emotionserkennung für natürliche Mensch-Roboter-Interaktion</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Ahmad</namePart>
                        <namePart type="family">Rabie</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Franz</namePart>
                        <namePart type="family">Kummert</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">dgs</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">doctoralThesis</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2010</dateIssued>
                        <dateOther encoding="w3cdtf" type="defenseDate">2011-01-31</dateOther>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">Since the computer has conquered our life and became an essential need rather than an accessory, more-sophisticated human-computer interaction, beyond the traditional keyboard, mouse, and monitor, is aimed to enable the users to interact with computers more socially. Emotional interaction play a major role in social life, thus the affective humanrobot interaction has evolved significantly throughout the last decades. The aim of this thesis is to provide the ability of emotion understanding for a robot. Throughout the thesis, a discrete theory of emotions is used as a frame of reference. According to it, emotions can be classified into some basic emotion classes.
The research is orginized around two goals. The first goal is to enable a robot to infer the emotional state of its interaction partner by analysing the displayed facial expression in non-constrained conditions. To achieve that, a robust, fully automatic, non-invasive, and real-time applicable vision-based system is developed with the ability to be implemented in the robot.
As the aim is to enable the robot to interact with its interactant in eal-world scenarios, sitautions in which the user is engaged in conversational sessions present farther challange for such systems. The second goal of this work is to combine facial expression and speech information cues in such a way, as to enable the affective system of the robot to fit such situations. En route to this goal possible affects of facial configuration related to speech on inferring emotions from facial expression is investigated. The results suggest a degraded performance when facial expressions are displyed during speech as displying them deliberately. In order to smooth this effect, information of audio signal is taken into account. The performance of the emotion recognition system is relatively enhanced by fusing facial expression cues and speech information ones into a bimodal system. The perfomance of the bimodal system still, however, degraded comparing with the perfomance stand-alone facial expression analyis system in the case of displayning facial expression deliberately.
Finally, the extent of recognizing each emotion by utilizng each modality is invistagted. The results indicate a highly varying performance of each modality ith the respective emotion class, and for the bimodal system, each modality should be weighted according to its discriminative power for a specific emotion.</abstract>
                    <subject>
                    </subject>
                    <classification authority="ddc">004</classification>
                    <identifier type="urn">urn:nbn:de:hbz:361-18396</identifier>
                    <identifier type="sys">HT016851695</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2305667</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>oaDoctoralThesis</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2305667">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2305667/2305670/Audio_Visual_Emotion_Recognition_Ahmad_Rabie_2011.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2305667" DMDID="DMD2305667">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2305667"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
