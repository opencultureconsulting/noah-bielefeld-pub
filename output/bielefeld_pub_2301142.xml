<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2301142">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Generation and evaluation of communicative robot gesture</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Maha</namePart>
                        <namePart type="family">Salem</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Stefan</namePart>
                        <namePart type="family">Kopp</namePart>
                        <nameIdentifier type="orcid" typeURI="http://orcid.org">0000-0002-4047-9277</nameIdentifier>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Ipke</namePart>
                        <namePart type="family">Wachsmuth</namePart>
                        <nameIdentifier type="orcid" typeURI="http://orcid.org">0000-0002-4786-5189</nameIdentifier>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Katharina</namePart>
                        <namePart type="family">Rohlfing</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Frank</namePart>
                        <namePart type="family">Joublin</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">article</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2012</dateIssued>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">How is communicative gesture behavior in robots perceived by humans? Although gesture is crucial in social interaction, this research question is still largely unexplored in the field of social robotics. Thus, the main objective of the present work is to investigate how gestural machine behaviors can be used to design more natural communication in social robots. The chosen approach is twofold. Firstly, the technical challenges encountered when implementing a speech-gesture generation model on a robotic platform are tackled. We present a framework that enables the humanoid robot to flexibly produce synthetic speech and co-verbal hand and arm gestures at run-time, while not being limited to a predefined repertoire of motor actions. Secondly, the achieved flexibility in robot gesture is exploited in controlled experiments. To gain a deeper understanding of how communicative robot gesture might impact and shape human perception and evaluation of human-robot interaction, we conducted a between-subjects experimental study using the humanoid robot in a joint task scenario. We manipulated the non-verbal behaviors of the robot in three experimental conditions, so that it would refer to objects by utilizing either (1) unimodal (i.e., speech only) utterances, (2) congruent multimodal (i.e., semantically matching speech and gesture) or (3) incongruent multimodal (i.e., semantically non-matching speech and gesture) utterances. Our findings reveal that the robot is evaluated more positively when non-verbal behaviors such as hand and arm gestures are displayed along with speech, even if they do not semantically match the spoken utterance.</abstract>
                    <subject>
                        <topic>Social Human-Robot Interaction</topic>
                        <topic>Multimodal Interaction and Conversational Skills</topic>
                        <topic>Robot Companions and Social Robots</topic>
                        <topic>Non-verbal Cues and Expressiveness</topic>
                    </subject>
                    <classification authority="ddc">006</classification>
                    <relatedItem type="host">
                        <titleInfo>
                            <title>International Journal of Social Robotics, Special Issue on Expectations, Intentions, and Actions</title>
                        </titleInfo>
                        <part>
                            <detail type="volume">
                                <number>4</number>
                            </detail>
                            <detail type="issue">
                                <number>2</number>
                            </detail>
                            <extent unit="page">
                                <start>201</start>
                                <end>217</end>
                            </extent>
                        </part>
                    </relatedItem>
                    <identifier type="issn">1875-4791</identifier>
                    <identifier type="eIssn">1875-4805</identifier>
                    <identifier type="ISI">000208894500008</identifier>
                    <identifier type="urn">urn:nbn:de:0070-pub-23011426</identifier>
                    <identifier type="doi">10.1007/s12369-011-0124-9</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2301142</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>oaArticle</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2301142">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2301142/2633181/SoRo_SalemEtAl.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2301142" DMDID="DMD2301142">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2301142"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
