<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2452128">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Automatic detection of motion sequences for motion analysis</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Bernhard-Andreas</namePart>
                        <namePart type="family">Br√ºning</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Christian</namePart>
                        <namePart type="family">Schnier</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Karola</namePart>
                        <namePart type="family">Pitsch</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Sven</namePart>
                        <namePart type="family">Wachsmuth</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Dirk</namePart>
                        <namePart type="family">Heylen</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Patrizia</namePart>
                        <namePart type="family">Paggio</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Michael</namePart>
                        <namePart type="family">Kipp</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">edt</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">conferenceObject</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2011</dateIssued>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="eng">In order to understand and model the non-verbal communicative behavior of humans, qualitative techniques, such as Conversation Analysis, and quantitative techniques, such as 3D motion capturing, need to be combined. Although there has been some recent progress in annotation tools like ELAN or Anvil, there is still a lack of appropriate tool support that enables a concise simultaneous access to both types of data and that shows the relationship between them. Within this work, we present a pre- annotation tool that takes the results from off-the-shelf optical tracking systems, automatically fits an articulated skeleton model, and detects motion segments of individual joints. A sophisticated user interface easily allows the annotating person to find correlations between different joints, analyze the corresponding 3D pose in a reconstructed virtual environment, and to export combined qualitative and quantitative annotations to standard annotation tools. Using this technique we are able to examine complex setups with three persons in tight conversion or largely unconstrained engagement situations of humans and robots.</abstract>
                    <subject>
                        <topic>annotation</topic>
                        <topic>Motion capturing</topic>
                        <topic>Motion segmentation</topic>
                    </subject>
                    <classification authority="ddc">006</classification>
                    <identifier type="urn">urn:nbn:de:0070-pub-24521285</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2452128</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>conference_object</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2452128">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2452128/2452136/Brueningetal_header.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2452128" DMDID="DMD2452128">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2452128"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
