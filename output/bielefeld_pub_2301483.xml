<mets:mets xmlns:mets="http://www.loc.gov/METS/" xmlns:mods="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mets:dmdSec ID="DMD2301483">
        <mets:mdWrap MIMETYPE="text/xml" MDTYPE="MODS">
            <mets:xmlData>
                <mods xmlns="http://www.loc.gov/mods/v3" version="3.7" xmlns:vl="http://visuallibrary.net/vl">
                    <titleInfo>
                        <title>Real-time automatic emotion recognition from speech</title>
                    </titleInfo>
                    <titleInfo type="alternative">
                        <title>Automatische Emotionserkennung aus Sprache in Echtzeit</title>
                    </titleInfo>
                    <name type="personal">
                        <namePart type="given">Thurid</namePart>
                        <namePart type="family">Vogt</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">aut</roleTerm>
                        </role>
                    </name>
                    <name type="personal">
                        <namePart type="given">Britta</namePart>
                        <namePart type="family">Wrede</namePart>
                        <role>
                            <roleTerm type="code" authority="marcrelator">dgs</roleTerm>
                        </role>
                    </name>
                    <typeOfResource>text</typeOfResource>
                    <genre authority="dini">doctoralThesis</genre>
                    <originInfo>
                        <dateIssued encoding="w3cdtf">2010</dateIssued>
                        <dateOther encoding="w3cdtf" type="defenseDate">2010-10-19</dateOther>
                    </originInfo>
                    <language>
                        <languageTerm type="code" authority="iso639-2b">eng</languageTerm>
                    </language>
                    <abstract lang="ger">In den vergangenen Jahren ist in der Mensch-Maschine-Kommunikation die Notwendigkeit, auf den emotionalen Zustand des Nutzers einzugehen, allgemein anerkannt worden. Um diesen automatisch zu erkennen, ist besonders Sprache in den Fokus gerückt. Bisher ging es dabei hauptsächlich um akademische und wenig anwendungsbezogene Untersuchungen, die auf im voraus aufgenommenen Datenbanken mit emotionaler Sprache beruhen. Die Anforderungen hierbei unterscheiden sich jedoch von denen der Online-Analyse, insbesondere sind im letzteren Fall die Bedingungen schwieriger und weniger vorhersagbar.
Diese Dissertation beschäftigt sich mit der automatischen Erkennung von Emotionen aus Sprache in Echtzeit anhand akustischer Merkmale. Dazu wurden zunächst Experimente auf bestehenden Datenbanken mit emotionaler Sprache durchgeführt, um geeignete Methoden zur Segmentierung, Merkmalsextraktion und Klassifikation des Sprachsignals zu finden. Geeignet heißt hierbei, dass die Methoden möglichst schnell und möglichst korrekt arbeiten. Um weitgehend allgemeingültige Ergebnisse zu erhalten, wurden die Experimente auf drei Datenbanken mit sehr unterschiedlichen Sprach- und Emotionstypen durchgeführt, nämlich der Berlin Datenbank mit Emotionaler Sprache, dem FAU Aibo Emotionscorpus und dem SmartKom Mobile Corpus, die sowohl gelesene als auch spontane Sprache sowie gespielte und natürliche Emotionen enthalten. Die bei diesen Experimenten gewonnenen Erkenntnisse wurden dazu verwendet, eine umfassende Sammlung von Werkzeugen und Programmen zur Online- und Offline-Emotionserkennung, genannt EmoVoice, zu implementieren.
Anhand von verschiedenen prototypischen Anwendungen und drei Benutzerstudien wurde die praktische Nutzbarkeit von EmoVoice, insbesondere auch durch externe Softwareentwickler, bewiesen. Weiterhin wurden vier Offline-Studien zur multimodalen Emotionserkennung durchgeführt, die akustische Merkmale mit Kontextinformation (Geschlecht), Biosignalen, Wortinformation und Mimik verbinden, da multimodale Erkennungsansätze eine höhere Erkennungsgenauigkeit versprechen.</abstract>
                    <abstract lang="eng">Recently, the importance of reacting to the emotional state of a user has been generally accepted in the field of human-computer interaction and especially speech has received increased focus as a modality from which to automatically deduct information on emotion. So far, mainly academic and not very application-oriented offline studies based on previously recorded and annotated databases with emotional speech were conducted. However, demands of online analysis differ from that of offline analysis, in particular, conditions are more challenging and less predictable.
Therefore, in this thesis, real-time automatic emotion recognition from acoustic features of speech was investigated. First, offline experiments were conducted to find suitable audio segmentation, feature extraction and classification algorithms. Suitable means in this context that they should be fast and at the same time give as correct results as possible. To be more general, results were obtained from three databases of different speech and emotion types, the Berlin Database of Emotional Speech, the FAU Aibo Emotion Corpus and the SmartKom Mobile Corpus, which include read and spontaneous speech as well as acted and spontaneous emotions. Results lead to the implementation of a collection of offline as well as online emotion recognition tools called EmoVoice.
This thesis also demonstrates the applicability of the framework and its usability for external software developers with the help of several applications and three user studies. Furthermore, four offline studies of multimodal emotion recognition combining acoustic information with context information (gender), bio signals, words and facial expressions are described, since an improved accuracy can be expected from multimodal analysis.</abstract>
                    <subject>
                        <topic>Gefühl</topic>
                        <topic>Features</topic>
                        <topic>Applications</topic>
                        <topic>Sprache</topic>
                        <topic>Merkmale</topic>
                        <topic>Anwendungen</topic>
                        <topic>Speech</topic>
                        <topic>Automatic emotion recognition</topic>
                        <topic>Automatische Spracherkennung</topic>
                        <topic>Automatische Emotionserkennung</topic>
                    </subject>
                    <classification authority="ddc">004</classification>
                    <identifier type="urn">urn:nbn:de:hbz:361-17782</identifier>
                    <identifier type="sys">HT016775433</identifier>
                    <accessCondition type="use and reproduction" xlink:href="https://rightsstatements.org/vocab/InC/1.0/">Urheberrechtsschutz</accessCondition>
                    <recordInfo>
                        <recordIdentifier>bielefeld_pub_2301483</recordIdentifier>
                    </recordInfo>
                    <extension>
                        <vl:doctype>oaDoctoralThesis</vl:doctype>
                    </extension>
                </mods>
            </mets:xmlData>
        </mets:mdWrap>
    </mets:dmdSec>
    <mets:fileSec>
        <mets:fileGrp USE="pdf upload">
            <mets:file MIMETYPE="application/pdf" ID="FILE0_bielefeld_pub_2301483">
                <mets:FLocat xlink:href="https://pub.uni-bielefeld.de/download/2301483/2301486/DissVogt.pdf" LOCTYPE="URL"/>
            </mets:file>
        </mets:fileGrp>
    </mets:fileSec>
    <mets:structMap TYPE="LOGICAL">
        <mets:div TYPE="document" ID="bielefeld_pub_2301483" DMDID="DMD2301483">
            <mets:fptr FILEID="FILE0_bielefeld_pub_2301483"/>
        </mets:div>
    </mets:structMap>
</mets:mets>
